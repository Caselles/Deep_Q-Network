{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import gym\n",
    "import theano\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import math\n",
    "import json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experience replay Class for DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EXPERIENCE REPLAY\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        \n",
    "        \"\"\"Define max length of memory and gamma\"\"\"\n",
    "        \n",
    "        \n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        \n",
    "        \n",
    "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
    "        \"\"\"Add experience to memory\"\"\"\n",
    "        \n",
    "        \n",
    "        self.memory.append([states, game_over])\n",
    "        #Delete the first experience if the memory is too long\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        \n",
    "        \n",
    "        \"\"\"Get the batch input and targets we will train on\"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        #length of memory vector\n",
    "        len_memory = len(self.memory)\n",
    "        \n",
    "        #number of actions in action space\n",
    "        num_actions = model.output_shape[-1]\n",
    "        \n",
    "        #states is an experience : [input_t_minus_1, action, reward, input_t],\n",
    "        #so memory[0] is state and memory[0][0][0].shape[1] is the size of the input\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        \n",
    "        #if batch_size<len_memory (it is mostly the case), \n",
    "        #then input is a matrix with batch_size rows and size of obs columns\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        \n",
    "        #targets is a matrix with batch_size rows and number of actions columns\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        \n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=inputs.shape[0])):\n",
    "            \n",
    "            #get experience number idx, idx being a random number in [0,length of memory]\n",
    "            #There are batch_size experiences that are drawn\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            \n",
    "            #Is the game over ? AKA done in gym\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            #The inputs of the NN are the state of the experience drawn\n",
    "            inputs[i:i+1] = state_t\n",
    "            \n",
    "            # There should be no target values for actions not taken.\n",
    "            # Thou shalt not correct actions not taken #deep\n",
    "            # model.predict(state_t)[0] is the vector of Q(state_t) for each action\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            \n",
    "            #Q_sa=max_a{Q(s,a)}\n",
    "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "            \n",
    "            # if game_over is True then the sequence is terminated \n",
    "            if game_over:  # if game_over is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # the target for this particular experience is : reward_t + gamma * max_a' Q(s', a')\n",
    "                # We know that you should have : \n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole on OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-19 15:27:26,588] Making new env: CartPole-v0\n",
      "[2016-12-19 15:27:26,594] Clearing 6 monitor files from previous run (because force=True was provided)\n",
      "[2016-12-19 15:27:26,852] Starting new video recorder writing to /Users/Syzygy/Desktop/MVA/Reinforcement_Learning/Projet/DQN/Deep_Q_Network/Deep_Q_Network/test/openaigym.video.0.4650.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/999 | Loss 12.2688 | Accumulated reward 23.0000\n",
      "Epoch 001/999 | Loss 11.9030 | Accumulated reward 22.0000\n",
      "Epoch 002/999 | Loss 23.0123 | Accumulated reward 44.0000\n",
      "Epoch 003/999 | Loss 5.6196 | Accumulated reward 10.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-19 15:27:31,252] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/Syzygy/Desktop/MVA/Reinforcement_Learning/Projet/DQN/Deep_Q_Network/Deep_Q_Network/test')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004/999 | Loss 11.4141 | Accumulated reward 17.0000\n"
     ]
    }
   ],
   "source": [
    "#Define the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "#PARAMETERS\n",
    "\n",
    "#learning rate\n",
    "#learning_rate=0.01\n",
    "\n",
    "#exploration parameter : need to improve that\n",
    "epsilon = .95\n",
    "\n",
    "#decay rate for epsilon\n",
    "decay_rate=0.85\n",
    "\n",
    "#Number of possible actions\n",
    "num_actions = env.action_space.n \n",
    "\n",
    "#Number of epochs of training : one epoch is a game ! It ends when you lose\n",
    "epoch = 5\n",
    "\n",
    "#Length of memory\n",
    "max_memory = 400000\n",
    "\n",
    "#Number of hidden units\n",
    "hidden_size = 200\n",
    "\n",
    "#Size of batch for training\n",
    "batch_size = 32\n",
    "\n",
    "#Accumulated reward over epoch\n",
    "acc_reward=0\n",
    "\n",
    "#shape of observations\n",
    "observation_shape = env.observation_space.shape[0]\n",
    "\n",
    "#start recording training part\n",
    "env.monitor.start('test',force=True,video_callable=lambda count: count % 50 == 0)\n",
    "#env.monitor.configure(video_callable=lambda count: False)\n",
    "#env.monitor.start('test',force=True)\n",
    "                  \n",
    "                  \n",
    "#Parameter C\n",
    "C=0\n",
    "\n",
    "#RMSProp optimizer\n",
    "#RMSprop=keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0, clipvalue=1)\n",
    "#Adam optimizer\n",
    "Adam=keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, clipvalue=1)\n",
    "\n",
    "#Define the current DNN\n",
    "model = Sequential()\n",
    "#first fully connected layer, activation RELU\n",
    "model.add(Dense(hidden_size, input_dim=observation_shape, activation='relu'))\n",
    "#second fully connected layer, activation RELU\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "#third fully connected layer, activation RELU\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "#last fully connected layer, output Q(s,a,theta)\n",
    "model.add(Dense(num_actions))\n",
    "#choose optimization parameters\n",
    "model.compile(optimizer=Adam, loss='mean_squared_error')\n",
    "\n",
    "#Define the target DNN\n",
    "target_model = Sequential()\n",
    "#first fully connected layer, activation RELU\n",
    "target_model.add(Dense(hidden_size, input_dim=observation_shape, activation='relu'))\n",
    "#second fully connected layer, activation RELU\n",
    "target_model.add(Dense(hidden_size, activation='relu'))\n",
    "#third fully connected layer, activation RELU\n",
    "target_model.add(Dense(hidden_size, activation='relu'))\n",
    "#last fully connected layer, output Q(s,a,theta)\n",
    "target_model.add(Dense(num_actions))\n",
    "#choose optimization parameters\n",
    "target_model.compile(optimizer=Adam, loss='mean_squared_error')\n",
    "\n",
    "# If you want to continue training from a previous model, just uncomment the line bellow\n",
    "#model.load_weights(\"model_cartpole\")\n",
    "#target_model.load_weights(\"model_cartpole\")\n",
    "\n",
    "\n",
    "# Initialize experience replay object\n",
    "exp_replay = ExperienceReplay(max_memory=max_memory)\n",
    "\n",
    "# Train boyyy\n",
    "#win_cnt = 0\n",
    "for e in range(epoch):\n",
    "    #set loss to zero\n",
    "    loss = 0.\n",
    "    \n",
    "    #set accumulated reward to 0\n",
    "    acc_reward = 0\n",
    "    \n",
    "    #Set C to zero\n",
    "    C=0\n",
    "    \n",
    "    #env.reset() : reset the environment, get first observation\n",
    "    input_t = env.reset()\n",
    "    input_t = input_t.reshape((1,observation_shape))\n",
    "    \n",
    "    #the game starts, so set game_over to False\n",
    "    game_over = False\n",
    "    \n",
    "    # get initial input\n",
    "    #input_t = env.observe()\n",
    "    #We already have it with reset\n",
    "    \n",
    "    #Decay of epsilon\n",
    "    if (e+1)%400==0:\n",
    "        epsilon = epsilon*decay_rate\n",
    "        if epsilon<0.05:\n",
    "            epsilon=0.05\n",
    "\n",
    "\n",
    "    #decay on learning rate\n",
    "    #if e%50==0:\n",
    "    #    learning_rate=learning_rate*(decay_rate**3)\n",
    "    #    if learning_rate<0.0001:\n",
    "    #        learning_rate=0.0001\n",
    "    #    model.compile(sgd(lr=learning_rate), \"mse\")\n",
    "\n",
    "    while not game_over:\n",
    "        \n",
    "        #set this state to be the last state\n",
    "        input_tm1 = input_t\n",
    "        \n",
    "        # get next action according to espilon-greedy policy\n",
    "        if np.random.rand() <= epsilon:\n",
    "            #exploration\n",
    "            action = np.random.randint(0, num_actions, size=1)[0]\n",
    "        else:\n",
    "            #exploitation\n",
    "            q = model.predict(input_tm1)\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        #apply action, get rewards and new state\n",
    "        input_t, reward, game_over, infodemerde = env.step(action)\n",
    "        input_t = input_t.reshape((1,observation_shape))\n",
    "        \n",
    "        \n",
    "        #Accumulate reward\n",
    "        acc_reward += reward\n",
    "\n",
    "        # store experience\n",
    "        exp_replay.remember([input_tm1, action, reward, input_t], game_over)\n",
    "        \n",
    "        #Create new target network every C updates, by cloning the current network\n",
    "        if C%50==0:\n",
    "            model.save_weights(\"model_cartpole_TARGET\", overwrite=True)\n",
    "            with open(\"model_cartpole_TARGET.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile) \n",
    "            target_model.load_weights(\"model_cartpole_TARGET\")\n",
    "            #print('LAAAA')\n",
    "            \n",
    "        #Increment C\n",
    "        C += 1\n",
    "        \n",
    "        # adapt model\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "\n",
    "        loss += model.train_on_batch(inputs, targets)\n",
    "    print(\"Epoch {:03d}/999 | Loss {:.4f} | Accumulated reward {:.4f}\".format(e, loss, acc_reward))\n",
    "\n",
    "env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save trained model weights and architecture, this will be used by the visualization code\n",
    "model.save_weights(\"model_cartpole\", overwrite=True)\n",
    "with open(\"model_cartpole.json\", \"w\") as outfile:\n",
    "    json.dump(model.to_json(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-01 17:59:02,346] Making new env: CartPole-v0\n",
      "[2016-12-01 17:59:02,353] Starting new video recorder writing to /Users/Syzygy/Desktop/MVA/Reinforcement Learning/DQN/test/openaigym.video.9.49841.video000000.mp4\n",
      "[2016-12-01 17:59:04,292] Starting new video recorder writing to /Users/Syzygy/Desktop/MVA/Reinforcement Learning/DQN/test/openaigym.video.9.49841.video000001.mp4\n",
      "[2016-12-01 17:59:07,156] Starting new video recorder writing to /Users/Syzygy/Desktop/MVA/Reinforcement Learning/DQN/test/openaigym.video.9.49841.video000008.mp4\n",
      "[2016-12-01 17:59:12,061] Starting new video recorder writing to /Users/Syzygy/Desktop/MVA/Reinforcement Learning/DQN/test/openaigym.video.9.49841.video000027.mp4\n",
      "[2016-12-01 17:59:20,295] Starting new video recorder writing to /Users/Syzygy/Desktop/MVA/Reinforcement Learning/DQN/test/openaigym.video.9.49841.video000064.mp4\n",
      "[2016-12-01 17:59:39,596] Starting new video recorder writing to /Users/Syzygy/Desktop/MVA/Reinforcement Learning/DQN/test/openaigym.video.9.49841.video000125.mp4\n",
      "[2016-12-01 18:00:02,574] Starting new video recorder writing to /Users/Syzygy/Desktop/MVA/Reinforcement Learning/DQN/test/openaigym.video.9.49841.video000216.mp4\n",
      "[2016-12-01 18:00:32,894] Starting new video recorder writing to /Users/Syzygy/Desktop/MVA/Reinforcement Learning/DQN/test/openaigym.video.9.49841.video000343.mp4\n",
      "[2016-12-01 18:01:09,383] Starting new video recorder writing to /Users/Syzygy/Desktop/MVA/Reinforcement Learning/DQN/test/openaigym.video.9.49841.video000512.mp4\n",
      "[2016-12-01 18:01:50,208] Starting new video recorder writing to /Users/Syzygy/Desktop/MVA/Reinforcement Learning/DQN/test/openaigym.video.9.49841.video000729.mp4\n",
      "[2016-12-01 18:02:41,120] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/Syzygy/Desktop/MVA/Reinforcement Learning/DQN/test')\n"
     ]
    }
   ],
   "source": [
    "# PLAYING PART\n",
    " \n",
    "with open(\"model_cartpole.json\", \"r\") as jfile:\n",
    "    model = model_from_json(json.load(jfile))\n",
    "model.load_weights(\"model_cartpole\")\n",
    "model.compile(\"sgd\", \"mse\")\n",
    "\n",
    "#Define the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "#start recording\n",
    "env.monitor.start('test',force=True)\n",
    "\n",
    "\n",
    "for e in range(1000):\n",
    "    \n",
    "    #set loss to 0\n",
    "    loss = 0.\n",
    "    \n",
    "    #env.reset() : reset the environment, get first observation\n",
    "    input_t = env.reset()\n",
    "    input_t = input_t.reshape((1,4))\n",
    "    \n",
    "    #Game starts so not over\n",
    "    game_over = False\n",
    "    \n",
    "    while not game_over:\n",
    "        \n",
    "        #set this state to be the last state\n",
    "        input_tm1 = input_t\n",
    "        \n",
    "        env.render()\n",
    "\n",
    "        # get next action\n",
    "        q = model.predict(input_tm1)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        input_t, reward, game_over, infodemerde = env.step(action)\n",
    "        input_t = input_t.reshape((1,4))\n",
    "        \n",
    "        \n",
    "env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-02 10:11:25,857] [CartPole-v0] Uploading 150 episodes of training data\n",
      "[2016-12-02 10:11:28,027] [CartPole-v0] Uploading videos of 6 training episodes (43709 bytes)\n",
      "[2016-12-02 10:11:29,263] [CartPole-v0] Creating evaluation object from /Users/Syzygy/Desktop/MVA/Reinforcement Learning/DQN/test with learning curve and training video\n",
      "[2016-12-02 10:11:29,937] \n",
      "****************************************************\n",
      "You successfully uploaded your evaluation on CartPole-v0 to\n",
      "OpenAI Gym! You can find it at:\n",
      "\n",
      "    https://gym.openai.com/evaluations/eval_gWrrMzaKS0GYcrRAUYsQWQ\n",
      "\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "#UPLOADING RESULTS\n",
    "\n",
    "gym.upload('/.../...', api_key='XXXXXXXXXXXXXXXX')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
